{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio --quiet\n",
        "\n",
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# Monter Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6L8P0ub1App",
        "outputId": "1d678ca9-05d5-40f6-bd3c-bd9151457c03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Chemin du mod√®le YOLOv5 best.pt sur Drive\n",
        "MODEL_PATH = \"/content/drive/MyDrive/YOLOv5_runs_finetuning2/fine_tuning/updated_model3/weights/best.pt\"\n",
        "\n",
        "# Charger le mod√®le YOLOv5\n",
        "detector = torch.hub.load('ultralytics/yolov5', 'custom', path=MODEL_PATH, force_reload=True)\n",
        "\n",
        "# D√©finir des couleurs uniques pour chaque classe\n",
        "class_colors = {\n",
        "    'Cap': (255, 0, 0),  # Rouge\n",
        "    'Defective-Label': (0, 255, 0),  # Vert\n",
        "    'Good-Label': (0, 0, 255),  # Bleu\n",
        "    'No-Cap': (255, 255, 0),  # Jaune\n",
        "    'No-label': (255, 0, 255),  # Magenta\n",
        "    'bottle': (0, 255, 255)  # Cyan\n",
        "}\n",
        "\n",
        "def detect_objects(image, confidence_threshold):\n",
        "    \"\"\"D√©tection d'objets avec affichage des scores sur les bounding boxes.\"\"\"\n",
        "    image_cv = np.array(image)\n",
        "    results = detector(image_cv)\n",
        "\n",
        "    for *box, conf, cls in results.xyxy[0]:\n",
        "        if conf < confidence_threshold:\n",
        "            continue  # Ignorer les d√©tections sous le seuil\n",
        "\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        class_name = detector.names[int(cls)]\n",
        "        color = class_colors.get(class_name, (255, 255, 255))  # Blanc si classe inconnue\n",
        "        label = f\"{class_name} {conf:.2f}\"\n",
        "\n",
        "        # Dessiner la bounding box\n",
        "        cv2.rectangle(image_cv, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Ajouter un fond pour am√©liorer la lisibilit√© du texte\n",
        "        (tw, th), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "        cv2.rectangle(image_cv, (x1, y1 - th - 5), (x1 + tw, y1), color, -1)\n",
        "\n",
        "        # Ajouter le texte de d√©tection avec score\n",
        "        cv2.putText(image_cv, label, (x1, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "    return Image.fromarray(image_cv)\n",
        "\n",
        "# Interface utilisateur avec curseur pour ajuster la confiance\n",
        "iface = gr.Interface(\n",
        "    fn=detect_objects,\n",
        "    inputs=[\n",
        "        gr.Image(type=\"pil\", label=\"üì∑ Uploader une image\"),\n",
        "        gr.Slider(0.0, 1.0, value=0.5, step=0.05, label=\"üîç Seuil de Confiance\"),\n",
        "    ],\n",
        "    outputs=gr.Image(type=\"pil\", label=\"üîé R√©sultat avec D√©tection\"),\n",
        "    title=\"D√©tection de Bouteilles avec YOLOv5\",\n",
        "    description=\"Uploader une image et ajuster le seuil de confiance pour filtrer les d√©tections.\",\n",
        ")\n",
        "\n",
        "# Lancer l'interface\n",
        "iface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "ASJ4o7azrY8Q",
        "outputId": "608b4cc0-12df-4d41-c676-310274a9f8ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n",
            "YOLOv5 üöÄ 2025-2-25 Python-3.11.11 torch-2.5.1+cu124 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model summary: 157 layers, 7026307 parameters, 0 gradients, 15.8 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://3e149ac863e7ea57f2.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://3e149ac863e7ea57f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}